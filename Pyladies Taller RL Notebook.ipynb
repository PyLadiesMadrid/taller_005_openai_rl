{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TALLER RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.policy import BoltzmannGumbelQPolicy , EpsGreedyQPolicy\n",
    "from rl.agents.dqn import DQNAgent \n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENVIRONMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FishingDerby-v0\")\n",
    "observation = env.reset()\n",
    "nb_actions = env.action_space.n\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1209612   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                182       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 18)                270       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 18)                0         \n",
      "=================================================================\n",
      "Total params: 1,210,274\n",
      "Trainable params: 1,210,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,)+env.observation_space.shape))\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "model.add(Dense(14, activation='sigmoid'))\n",
    "model.add(Dense(14, activation='sigmoid'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### POLICY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=10000,window_length=1)\n",
    "policy = EpsGreedyQPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=50,\n",
    "              memory=memory,target_model_update=1e-2,policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "  2582/100000: episode: 1, duration: 119.734s, episode steps: 2582, steps per second: 22, episode reward: -99.000, mean reward: -0.038 [-1.000, 0.000], mean action: 9.089 [0.000, 17.000], mean observation: 71.056 [0.000, 240.000], loss: 0.019236, acc: 0.102887, mean_absolute_error: 0.054796, mean_q: 0.102930\n",
      "  4867/100000: episode: 2, duration: 103.366s, episode steps: 2285, steps per second: 22, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 10.395 [0.000, 17.000], mean observation: 71.100 [0.000, 240.000], loss: 0.018506, acc: 0.080621, mean_absolute_error: 0.054582, mean_q: 0.087650\n",
      "  7713/100000: episode: 3, duration: 130.024s, episode steps: 2846, steps per second: 22, episode reward: -87.000, mean reward: -0.031 [-1.000, 1.000], mean action: 10.062 [0.000, 17.000], mean observation: 71.202 [0.000, 240.000], loss: 0.020166, acc: 0.071943, mean_absolute_error: 0.055025, mean_q: 0.074873\n",
      "  9850/100000: episode: 4, duration: 99.115s, episode steps: 2137, steps per second: 22, episode reward: -93.000, mean reward: -0.044 [-1.000, 1.000], mean action: 9.257 [0.000, 17.000], mean observation: 71.098 [0.000, 240.000], loss: 0.020580, acc: 0.056519, mean_absolute_error: 0.055222, mean_q: 0.069919\n",
      " 12034/100000: episode: 5, duration: 97.721s, episode steps: 2184, steps per second: 22, episode reward: -93.000, mean reward: -0.043 [-1.000, 1.000], mean action: 7.390 [0.000, 17.000], mean observation: 71.085 [0.000, 240.000], loss: 0.021506, acc: 0.051912, mean_absolute_error: 0.055217, mean_q: 0.067962\n",
      " 14315/100000: episode: 6, duration: 101.688s, episode steps: 2281, steps per second: 22, episode reward: -97.000, mean reward: -0.043 [-1.000, 1.000], mean action: 10.857 [0.000, 17.000], mean observation: 71.126 [0.000, 240.000], loss: 0.021627, acc: 0.054074, mean_absolute_error: 0.055689, mean_q: 0.078275\n",
      " 16850/100000: episode: 7, duration: 111.563s, episode steps: 2535, steps per second: 23, episode reward: -99.000, mean reward: -0.039 [-1.000, 0.000], mean action: 9.133 [0.000, 17.000], mean observation: 71.078 [0.000, 240.000], loss: 0.022376, acc: 0.059504, mean_absolute_error: 0.055524, mean_q: 0.071911\n",
      " 19116/100000: episode: 8, duration: 102.446s, episode steps: 2266, steps per second: 22, episode reward: -99.000, mean reward: -0.044 [-1.000, 0.000], mean action: 8.611 [0.000, 17.000], mean observation: 71.179 [0.000, 240.000], loss: 0.022027, acc: 0.056308, mean_absolute_error: 0.055556, mean_q: 0.072290\n",
      " 21861/100000: episode: 9, duration: 128.560s, episode steps: 2745, steps per second: 21, episode reward: -95.000, mean reward: -0.035 [-1.000, 1.000], mean action: 6.239 [0.000, 17.000], mean observation: 71.119 [0.000, 240.000], loss: 0.020287, acc: 0.052926, mean_absolute_error: 0.055378, mean_q: 0.070969\n",
      " 24220/100000: episode: 10, duration: 106.262s, episode steps: 2359, steps per second: 22, episode reward: -99.000, mean reward: -0.042 [-1.000, 0.000], mean action: 9.341 [0.000, 17.000], mean observation: 71.125 [0.000, 240.000], loss: 0.019723, acc: 0.049928, mean_absolute_error: 0.055410, mean_q: 0.071526\n",
      " 26502/100000: episode: 11, duration: 100.861s, episode steps: 2282, steps per second: 23, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 11.652 [0.000, 17.000], mean observation: 71.182 [0.000, 240.000], loss: 0.019664, acc: 0.041014, mean_absolute_error: 0.055317, mean_q: 0.068658\n",
      " 28832/100000: episode: 12, duration: 104.403s, episode steps: 2330, steps per second: 22, episode reward: -95.000, mean reward: -0.041 [-1.000, 1.000], mean action: 6.829 [0.000, 17.000], mean observation: 71.113 [0.000, 240.000], loss: 0.020724, acc: 0.037004, mean_absolute_error: 0.055599, mean_q: 0.070645\n",
      " 31282/100000: episode: 13, duration: 109.308s, episode steps: 2450, steps per second: 22, episode reward: -99.000, mean reward: -0.040 [-1.000, 0.000], mean action: 8.660 [0.000, 17.000], mean observation: 71.187 [0.000, 240.000], loss: 0.020576, acc: 0.048240, mean_absolute_error: 0.055251, mean_q: 0.064984\n",
      " 33659/100000: episode: 14, duration: 104.732s, episode steps: 2377, steps per second: 23, episode reward: -99.000, mean reward: -0.042 [-1.000, 0.000], mean action: 4.419 [0.000, 17.000], mean observation: 71.038 [0.000, 240.000], loss: 0.021371, acc: 0.068101, mean_absolute_error: 0.055801, mean_q: 0.071977\n",
      " 35938/100000: episode: 15, duration: 102.527s, episode steps: 2279, steps per second: 22, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 7.996 [0.000, 17.000], mean observation: 71.026 [0.000, 240.000], loss: 0.021108, acc: 0.033129, mean_absolute_error: 0.055838, mean_q: 0.072888\n",
      " 38290/100000: episode: 16, duration: 106.121s, episode steps: 2352, steps per second: 22, episode reward: -95.000, mean reward: -0.040 [-1.000, 1.000], mean action: 12.774 [0.000, 17.000], mean observation: 71.060 [0.000, 240.000], loss: 0.020343, acc: 0.046264, mean_absolute_error: 0.055589, mean_q: 0.071519\n",
      " 40576/100000: episode: 17, duration: 102.496s, episode steps: 2286, steps per second: 22, episode reward: -97.000, mean reward: -0.042 [-1.000, 1.000], mean action: 7.648 [0.000, 17.000], mean observation: 71.107 [0.000, 240.000], loss: 0.020527, acc: 0.033383, mean_absolute_error: 0.055799, mean_q: 0.073435\n",
      " 43225/100000: episode: 18, duration: 121.942s, episode steps: 2649, steps per second: 22, episode reward: -95.000, mean reward: -0.036 [-1.000, 1.000], mean action: 8.116 [0.000, 17.000], mean observation: 71.144 [0.000, 240.000], loss: 0.020800, acc: 0.048167, mean_absolute_error: 0.055691, mean_q: 0.072200\n",
      " 46464/100000: episode: 19, duration: 145.510s, episode steps: 3239, steps per second: 22, episode reward: -99.000, mean reward: -0.031 [-1.000, 0.000], mean action: 5.533 [0.000, 17.000], mean observation: 71.028 [0.000, 240.000], loss: 0.020062, acc: 0.052466, mean_absolute_error: 0.055660, mean_q: 0.074359\n",
      " 48733/100000: episode: 20, duration: 105.791s, episode steps: 2269, steps per second: 21, episode reward: -99.000, mean reward: -0.044 [-1.000, 0.000], mean action: 9.366 [0.000, 17.000], mean observation: 71.009 [0.000, 240.000], loss: 0.018627, acc: 0.033247, mean_absolute_error: 0.055494, mean_q: 0.073501\n",
      " 51021/100000: episode: 21, duration: 104.988s, episode steps: 2288, steps per second: 22, episode reward: -97.000, mean reward: -0.042 [-1.000, 1.000], mean action: 9.797 [0.000, 17.000], mean observation: 71.081 [0.000, 240.000], loss: 0.018357, acc: 0.036426, mean_absolute_error: 0.055183, mean_q: 0.069675\n",
      " 54089/100000: episode: 22, duration: 133.386s, episode steps: 3068, steps per second: 23, episode reward: -81.000, mean reward: -0.026 [-1.000, 1.000], mean action: 8.507 [0.000, 17.000], mean observation: 71.173 [0.000, 240.000], loss: 0.018180, acc: 0.054362, mean_absolute_error: 0.055519, mean_q: 0.077644\n",
      " 56377/100000: episode: 23, duration: 99.310s, episode steps: 2288, steps per second: 23, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 6.422 [0.000, 17.000], mean observation: 70.991 [0.000, 240.000], loss: 0.019268, acc: 0.037888, mean_absolute_error: 0.055232, mean_q: 0.068035\n",
      " 58652/100000: episode: 24, duration: 98.476s, episode steps: 2275, steps per second: 23, episode reward: -97.000, mean reward: -0.043 [-1.000, 1.000], mean action: 6.346 [0.000, 17.000], mean observation: 70.992 [0.000, 240.000], loss: 0.020839, acc: 0.045041, mean_absolute_error: 0.055403, mean_q: 0.068904\n",
      " 60979/100000: episode: 25, duration: 100.817s, episode steps: 2327, steps per second: 23, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 11.742 [0.000, 17.000], mean observation: 71.128 [0.000, 240.000], loss: 0.019949, acc: 0.032378, mean_absolute_error: 0.055497, mean_q: 0.070772\n",
      " 63283/100000: episode: 26, duration: 100.270s, episode steps: 2304, steps per second: 23, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 11.730 [0.000, 17.000], mean observation: 71.259 [0.000, 240.000], loss: 0.021373, acc: 0.035536, mean_absolute_error: 0.055692, mean_q: 0.070742\n",
      " 65658/100000: episode: 27, duration: 117.051s, episode steps: 2375, steps per second: 20, episode reward: -87.000, mean reward: -0.037 [-1.000, 1.000], mean action: 6.950 [0.000, 17.000], mean observation: 71.118 [0.000, 240.000], loss: 0.022033, acc: 0.039566, mean_absolute_error: 0.056161, mean_q: 0.077318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68341/100000: episode: 28, duration: 115.881s, episode steps: 2683, steps per second: 23, episode reward: -99.000, mean reward: -0.037 [-1.000, 0.000], mean action: 5.626 [0.000, 17.000], mean observation: 71.067 [0.000, 240.000], loss: 0.021101, acc: 0.056571, mean_absolute_error: 0.055985, mean_q: 0.077998\n",
      " 70457/100000: episode: 29, duration: 91.163s, episode steps: 2116, steps per second: 23, episode reward: -99.000, mean reward: -0.047 [-1.000, 0.000], mean action: 6.914 [0.000, 17.000], mean observation: 71.000 [0.000, 240.000], loss: 0.021085, acc: 0.035932, mean_absolute_error: 0.056045, mean_q: 0.075941\n",
      " 72785/100000: episode: 30, duration: 100.304s, episode steps: 2328, steps per second: 23, episode reward: -95.000, mean reward: -0.041 [-1.000, 1.000], mean action: 7.759 [0.000, 17.000], mean observation: 71.100 [0.000, 240.000], loss: 0.020372, acc: 0.036593, mean_absolute_error: 0.055785, mean_q: 0.073796\n",
      " 75103/100000: episode: 31, duration: 99.918s, episode steps: 2318, steps per second: 23, episode reward: -95.000, mean reward: -0.041 [-1.000, 1.000], mean action: 10.789 [0.000, 17.000], mean observation: 71.285 [0.000, 240.000], loss: 0.020346, acc: 0.043073, mean_absolute_error: 0.055693, mean_q: 0.074035\n",
      " 77502/100000: episode: 32, duration: 103.441s, episode steps: 2399, steps per second: 23, episode reward: -95.000, mean reward: -0.040 [-1.000, 1.000], mean action: 7.433 [0.000, 17.000], mean observation: 71.005 [0.000, 240.000], loss: 0.021274, acc: 0.048927, mean_absolute_error: 0.055519, mean_q: 0.068395\n",
      " 79768/100000: episode: 33, duration: 98.136s, episode steps: 2266, steps per second: 23, episode reward: -99.000, mean reward: -0.044 [-1.000, 0.000], mean action: 6.261 [0.000, 17.000], mean observation: 71.136 [0.000, 240.000], loss: 0.021411, acc: 0.040545, mean_absolute_error: 0.055827, mean_q: 0.072152\n",
      " 82476/100000: episode: 34, duration: 127.087s, episode steps: 2708, steps per second: 21, episode reward: -99.000, mean reward: -0.037 [-1.000, 0.000], mean action: 9.516 [0.000, 17.000], mean observation: 71.107 [0.000, 240.000], loss: 0.020850, acc: 0.043678, mean_absolute_error: 0.055688, mean_q: 0.070331\n",
      " 84824/100000: episode: 35, duration: 105.283s, episode steps: 2348, steps per second: 22, episode reward: -95.000, mean reward: -0.040 [-1.000, 1.000], mean action: 12.298 [0.000, 17.000], mean observation: 71.152 [0.000, 240.000], loss: 0.020557, acc: 0.037652, mean_absolute_error: 0.055700, mean_q: 0.072763\n",
      " 87129/100000: episode: 36, duration: 101.000s, episode steps: 2305, steps per second: 23, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 9.752 [0.000, 17.000], mean observation: 71.291 [0.000, 240.000], loss: 0.019972, acc: 0.037798, mean_absolute_error: 0.055543, mean_q: 0.071127\n",
      " 89534/100000: episode: 37, duration: 106.685s, episode steps: 2405, steps per second: 23, episode reward: -97.000, mean reward: -0.040 [-1.000, 1.000], mean action: 4.562 [0.000, 17.000], mean observation: 71.056 [0.000, 240.000], loss: 0.020260, acc: 0.048077, mean_absolute_error: 0.056050, mean_q: 0.078093\n",
      " 91822/100000: episode: 38, duration: 103.800s, episode steps: 2288, steps per second: 22, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 8.108 [0.000, 17.000], mean observation: 70.973 [0.000, 240.000], loss: 0.019719, acc: 0.037505, mean_absolute_error: 0.055708, mean_q: 0.074056\n",
      " 94106/100000: episode: 39, duration: 107.195s, episode steps: 2284, steps per second: 21, episode reward: -97.000, mean reward: -0.042 [-1.000, 1.000], mean action: 9.492 [0.000, 17.000], mean observation: 71.172 [0.000, 240.000], loss: 0.020728, acc: 0.031455, mean_absolute_error: 0.055908, mean_q: 0.075177\n",
      " 96397/100000: episode: 40, duration: 107.382s, episode steps: 2291, steps per second: 21, episode reward: -99.000, mean reward: -0.043 [-1.000, 0.000], mean action: 7.633 [0.000, 17.000], mean observation: 71.027 [0.000, 240.000], loss: 0.020692, acc: 0.035165, mean_absolute_error: 0.055823, mean_q: 0.074583\n",
      " 98731/100000: episode: 41, duration: 115.619s, episode steps: 2334, steps per second: 20, episode reward: -93.000, mean reward: -0.040 [-1.000, 1.000], mean action: 9.043 [0.000, 17.000], mean observation: 71.325 [0.000, 240.000], loss: 0.021309, acc: 0.037918, mean_absolute_error: 0.056200, mean_q: 0.082120\n",
      "done, took 4541.918 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122b61710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['acc', 'mae'])\n",
    "dqn.fit(env, nb_steps=100000, visualize=True,  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -99.000, steps: 2287\n",
      "Episode 2: reward: -99.000, steps: 2318\n",
      "Episode 3: reward: -99.000, steps: 2303\n",
      "Episode 4: reward: -99.000, steps: 2304\n",
      "Episode 5: reward: -99.000, steps: 2304\n",
      "Episode 6: reward: -99.000, steps: 2283\n",
      "Episode 7: reward: -99.000, steps: 2309\n",
      "Episode 8: reward: -99.000, steps: 2291\n",
      "Episode 9: reward: -99.000, steps: 2292\n",
      "Episode 10: reward: -99.000, steps: 2287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122223e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
